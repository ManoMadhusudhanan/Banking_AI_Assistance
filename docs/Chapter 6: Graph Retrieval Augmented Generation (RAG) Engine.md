# Chapter 5: Retrieval-Augmented Generation (RAG) Engine

In Chapter 4: OCR (Optical Character Recognition) Utility, we saw how our Banking_AI_Assistance gained "eyes" to read text from images. Now, our bot can "see" your questions (whether typed text or text from an image) and (in Chapter 3: Intent Detection System) even "understand" what you're trying to do. But understanding isn't enough; the bot needs to provide accurate and factual answers, especially in banking where precision is key.

This is where the Retrieval-Augmented Generation (RAG) Engine becomes incredibly important. It's the component that gives our chatbot the ability to be a smart "research assistant" and ensures its answers are always grounded in real banking facts.

---

## What Problem Does It Solve?

Imagine you ask our AI banking assistant:

> "What is the current interest rate for a 5-year fixed deposit account?"

You might think a very smart AI, like a Large Language Model (LLM) (which we'll discuss in Chapter 7: Large Language Model (LLM) Service), could just answer this. And it could! But here's the challenge:

- **Factuality:** The LLM might have learned general knowledge about fixed deposits, but it might not know your specific bank's current interest rates. It could even "hallucinate" (make up) a rate that sounds plausible but is completely wrong.
- **Up-to-dateness:** Banking policies and rates change frequently. The LLM's general training data might be old.
- **Specificity:** The LLM might not know the exact details of your bank's different fixed deposit products.

If our AI assistant gives you wrong information about interest rates, it could lead to serious problems. This is where the RAG Engine steps in.

---

## How the RAG Engine Helps

The RAG Engine acts like a highly specialized research assistant. When the bot gets a factual question, the RAG Engine doesn't guess. Instead, it:

- **Searches:** Dives into your bank's official documents (like a digital library of all banking policies, product descriptions, FAQs).
- **Finds:** Retrieves only the most relevant pieces of information that directly answer your question.
- **Provides:** Gives these factual snippets to the main chatbot (LLM) to craft an accurate and grounded response.

It's like having a dedicated, super-fast librarian who knows exactly which book and page contain the answer you need, and then hands those pages to a brilliant speaker to explain it clearly. This ensures responses are precise, up-to-date, and free from made-up information.

---

## Key Concepts

Let's break down how this smart "research assistant" works.

---

### 1. What is RAG? (Retrieval-Augmented Generation)

RAG stands for Retrieval-Augmented Generation. It combines two powerful ideas:

- **Retrieval:** The ability to find relevant information from a huge collection of documents (like searching a library).
- **Augmented Generation:** The ability to use that found information to improve (augment) the answer generated by a smart AI (like an LLM).

Think of our RAG Engine as having two main jobs:

| RAG Job | Analogy | What it does for our Banking AI |
|-------|---------|----------------------------------|
| Retrieval | The "Librarian" | Finds the most relevant banking documents or snippets for a query |
| Augmentation | The "Smart Speaker" | Gives those snippets to the main AI to help it generate an accurate, fact-checked answer |

---

### 2. The Knowledge Base (Our Bank's Library)

The heart of the RAG system is its knowledge base. This is where all the factual information lives. For our Banking_AI_Assistance, this means:

- Internal banking policy documents  
- Product brochures and descriptions  
- Official FAQs  
- Regulatory guidelines  

This "library" is not just a bunch of files. It's organized in a special way to make searching very fast and smart.

---

### 3. Embeddings: Text "Fingerprints"

How does the RAG Engine know which parts of the knowledge base are "relevant"? It uses embeddings.

- An embedding is a special numerical "fingerprint" for a piece of text (a word, a sentence, or a document chunk).
- Texts with similar meanings will have very similar numerical fingerprints.

When you ask a question, the RAG Engine creates a fingerprint for your question. Then, it quickly finds all the banking document fingerprints that are most similar. This is how it identifies relevant information based on meaning, not just keywords.

---

### 4. Vector Database (Qdrant): The Super-Fast Catalog

To store and quickly search through millions of these numerical "fingerprints" (embeddings), RAG systems use a special type of database called a Vector Database.

- Our project uses **Qdrant** as its vector database.
- Think of Qdrant as a super-advanced library catalog.
- It can instantly find the top document chunks whose meaning is closest to your question's meaning.

---

### 5. Chunking: Breaking Down Big Books

Before documents are added to the knowledge base, they are broken down into smaller pieces called chunks. This allows the RAG Engine to return only the relevant portion instead of an entire document.

---

## How to Use It (The `rag_search` Function)

The Orchestrator uses a function called `rag_search` to retrieve relevant information from the RAG Engine.

```python
# main.py - inside the chatbot function

if intent is not None or is_probable_banking or image_uploaded:
    use_rag = True

    if use_rag:
        context_chunks = rag_search(final_query)
        context = "\n".join(context_chunks) if isinstance(context_chunks, list) else context_chunks

        if not context or len(context.strip()) < 50:
            reply = groq_chat(session_id, final_query)
        else:
            prompt = f"..."
            answer = groq_chat(session_id, prompt)
```

The rag_search function takes your cleaned-up question (`final_query`) as input and returns a list of relevant text chunks from the banking knowledge base. These chunks are then used to "augment" the prompt for the[ Large Language Model (LLM) Service](https://github.com/ManoMadhusudhanan/Banking_AI_Assistance/blob/main/docs/Chapter%207%3A%20Large%20Language%20Model%20(LLM)%20Integration.md), helping it generate a more accurate answer.

Let's look at an example:

| Input (`final_query`) | Output (`context_chunks`) |
|----------------------|---------------------------|
| what documents do I need to open a savings account | - To open a savings account, you generally need ID (Aadhaar, Passport) and Address Proof (Utility Bill, Driver's License).<br>- A minimum initial deposit is required for opening new savings accounts as per bank policy. |
| how to apply for a personal loan | - Personal loans can be applied online via our website or by visiting any branch.<br>- Eligibility criteria include a stable income, good credit score, and minimum age of 21. |
| what are the charges for ATM withdrawals | - First 5 ATM withdrawals per month are free. Subsequent withdrawals incur a charge of Rs. 20 + GST.<br>- International ATM withdrawals have different fee structures. |

## Under the Hood: How the RAG Engine Works
Let's peek behind the curtain to see how the RAG Engine performs its research.

### The Flow of a RAG Search
Here's a simplified sequence of what happens when the Orchestrator asks the RAG Engine for information:

<img width="70%" height="589" alt="image" src="https://github.com/user-attachments/assets/11cd52af-104c-43df-944b-103a93875476" />


### 1. Setting Up the Banking "Library" (Building the RAG Index)
Before the RAG Engine can answer questions, it needs to build its knowledge base (the RAG index). This is usually done once, or whenever new documents are added, by calling the `build_rag_index` function in `rag_engine`.py.
```
# rag_engine.py - Simplified `build_rag_index`

from langchain_community.document_loaders import UnstructuredFileLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from sentence_transformers import SentenceTransformer
from qdrant_client.http import models as rest
import uuid

embedder = SentenceTransformer("all-MiniLM-L6-v2") # Our embedding model
# qdrant_client is initialized here (not shown for brevity)

def build_rag_index(source_file: str):
    # 1. Load banking documents (e.g., a DOCX file)
    loader = UnstructuredFileLoader(source_file)
    docs = loader.load()

    # 2. Break documents into smaller 'chunks' (pages/paragraphs)
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=300, # Each piece is around 300 characters
        chunk_overlap=50 # Pieces can slightly overlap for better context
    )
    chunks = splitter.split_documents(docs)
    texts = [c.page_content.strip() for c in chunks]

    # 3. Create numerical 'fingerprints' (embeddings) for each chunk
    vectors = embedder.encode(texts, batch_size=32)

    # 4. Prepare chunks and their fingerprints to be stored in Qdrant
    points = []
    for i, text in enumerate(texts):
        chunk_id = str(uuid.uuid4()) # Unique ID for each chunk
        points.append(
            rest.PointStruct(
                id=i, # An ID for Qdrant
                vector=vectors[i].tolist(), # The numerical fingerprint
                payload={ # Extra info to store with the chunk
                    "chunk_id": chunk_id,
                    "text": text,
                    # "entities": entities (simplified for beginner focus)
                }
            )
        )

    # 5. Store all chunks and their fingerprints in Qdrant
    qdrant_client.upsert(collection_name=COLLECTION, points=points)
    print("âœ… Vector RAG index built successfully!")
```
This `build_rag_index` function is essential for preparing the knowledge base:

- `UnstructuredFileLoader`: This helps read various document types, like Word files (.docx), PDF, etc.
- `RecursiveCharacterTextSplitter`: This intelligent tool breaks down long documents into smaller chunks, making retrieval more precise.
- `SentenceTransformer`: This model (all-`MiniLM`-L6-v2) is our "embedding brain" that converts text into numerical fingerprints.
- `embedder.encode(texts)`: This takes all our text chunks and generates their unique numerical vectors (fingerprints).
- `qdrant_client.upsert(...)`: This command sends all our text chunks along with their numerical fingerprints to the Qdrant Vector Database, making them searchable by meaning.
 
### 2. Performing a Research Query (rag_search)
When the Orchestrator calls `rag_search` with your question, here's a simplified view of what happens inside `rag_engine.py`:
```
# rag_engine.py - Simplified `rag_search`

from sentence_transformers import SentenceTransformer, util
import numpy as np

embedder = SentenceTransformer("all-MiniLM-L6-v2") # Our embedding model
# qdrant_client is initialized here (not shown for brevity)

# Helper function to calculate similarity between two fingerprints
def cosine_similarity(a, b):
    return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))

def rag_search(query: str, top_k: int = 4):
    # 1. Turn the user's question into a numerical 'fingerprint'
    query_vec = embedder.encode(query)

    # 2. Ask Qdrant to find document chunks with similar fingerprints
    search_results = qdrant_client.query_points(
        collection_name=COLLECTION,
        query=query_vec.tolist(), # The fingerprint of our question
        limit=top_k * 4, # Get more results initially to re-rank
        with_payload=True # Get the actual text back
    )

    if not search_results or not search_results.points:
        return None # No relevant documents found

    # 3. Re-rank the found chunks to ensure the most relevant ones are at the top
    scored_chunks = []
    for p in search_results.points:
        doc_text = p.payload["text"]
        doc_vec = embedder.encode(doc_text) # Get embedding for the retrieved document text
        sim = cosine_similarity(query_vec, doc_vec) # Calculate similarity
        if sim >= 0.18: # Only keep chunks with a good similarity score
            scored_chunks.append((sim, doc_text))

    scored_chunks.sort(reverse=True, key=lambda x: x[0]) # Sort by highest similarity

    # Note: Our project also includes advanced "Graph Boosting" here
    # (using Neo4j) to find even more interconnected context.
    # For simplicity, we are focusing on the core vector search.

    # 4. Return the top 'k' most relevant text chunks
    return [text for _, text in scored_chunks[:top_k]]
```
Here's how `rag_search` works its magic:

- `embedder.encode(query)`: Your question is instantly converted into a numerical fingerprint.
- `qdrant_client.query_points(...)`: This is the crucial step where the RAG Engine sends your question's fingerprint to `Qdrant`. `Qdrant` then speedily scans its vast collection of banking document fingerprints and returns the chunks that are semantically (by meaning) most similar.
- `cosine_similarity`: Even after getting initial results from Qdrant, the system performs an extra step called "re-ranking." It calculates the exact similarity of each retrieved chunk against your original query. This ensures that only the very best and most relevant pieces of information are passed on.
- `return [text for _, text in scored_chunks[:top_k]]`: Finally, the function returns a list of the top_k (e.g., 4) most relevant banking text snippets.
  
These retrieved chunks are the "facts" that will be given to the main AI (Large Language Model (LLM) Service) to help it craft an accurate and specific answer to your banking question.

## Conclusion
We've now seen how the **Graph Retrieval-Augmented Generation (RAG)** Engine acts as our Banking_AI_Assistance's intelligent "research assistant." It creates a factual foundation by storing banking documents in a specialized Vector Database (`Qdrant`) as numerical "fingerprints" (`embeddings`). When you ask a question, the RAG Engine efficiently retrieves the most relevant snippets from this knowledge base, ensuring that the bot's responses are accurate, up-to-date, and grounded in real banking information, preventing "made-up" answers.

With our bot now able to "see" (`OCR`), "understand" (`Intent Detection`), and "research" (`RAG`), the next logical step is to combine all this information to generate a truly helpful and human-like response. That's the job of the [Large Language Model (LLM) Service](https://github.com/ManoMadhusudhanan/Banking_AI_Assistance/blob/main/docs/Chapter%207%3A%20Large%20Language%20Model%20(LLM)%20Integration.md), which we'll explore in detail in a later chapter. First, we'll examine how specific banking rules and logic are handled directly by the orchestrator.

Next Chapter: [Large Language Model (LLM) Service](https://github.com/ManoMadhusudhanan/Banking_AI_Assistance/blob/main/docs/Chapter%207%3A%20Large%20Language%20Model%20(LLM)%20Integration.md)
